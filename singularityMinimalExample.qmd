---
title: "Singularity with R"
author: "Carl Beuchel"
date: today
output:
  html_document:
  code_download: true
theme: spacelab #sandstone #flatfly #spacelab
highlight: pygments
toc: true  
toc-depth: 3
code-folding: show
number-sections: true
toc-float:
  smooth-scroll: true
editor: source
editor-options: 
  chunk-output-type: console
execute:
  eval: false
  echo: true
  warning: false
---



```{r setup}
#| include: false
#| echo: false
#| eval: false

# set a more recent R snapshot as source repo
r = getOption("repos") 
r["CRAN"] = "https://mran.microsoft.com/snapshot/2022-07-01"
options(repos = r)
rm(r)

# Update packages to that snapshot
update.packages(
  ask = FALSE, 
  checkBuilt = TRUE
)

# Check unsuccessful updates packages
old.packages()

# Update V8
# Sys.setenv(DOWNLOAD_STATIC_LIBV8=1)
# Sys.getenv("DOWNLOAD_STATIC_LIBV8") # To get around V8 installation
# install.packages("V8")

# This needed a CXX17 definition in the Makevars
# install.packages("Boom") # still fails

# needed sudo R CMD rjavareconf 
# install.packages("rJava")

# install.packages("qs")
# install.packages("rstanarm")

# Install SingularityCE locally
pacman -S go squashfs-tools
```

## Setup

### Installation

Installation instructions for Singularity can be found in the [Singularity User
Guide](https://docs.sylabs.io/guides/3.10/user-guide/quick_start.html) and the
[Singularity Admin
Guide](https://docs.sylabs.io/guides/3.10/admin-guide/admin_quickstart.html).
Following the steps listed under `Quick Start`, it is relatively simple to build
Singularity from source.

The command `singularity buildcfg` shows the build configuration and display all
paths set during the installation. The build can also be tested from the
`builddir` of the singularity directory. The possible tests are documented
[here](https://docs.sylabs.io/guides/latest/admin-guide/installation.html#test-suite).


### Add local binary to secure_path

When building singularity locally, the binary is found in the path
`/usr/local/bin/`. To be able to build containers, they have to be run as root,
but sometimes the `/usr/local/bin/` directory is not in `$PATH` for root,
leading to a command not found error when running `sudo singularity`. Therefore
it is necessary to append the directory to `secure_path` of the `/etc/sudoers`
file. To do this, run the following command:

```{r}
#| eval: false

# edit the /etc/sudoers file with this command
sudo visudo
```

Search for a line that specifies the `secure_path`. It might look like this:
`Defaults secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"`.
Sometimes it might be commented out with a `#`. Now add the directory containing
the singularity binary by adding it with a prepended `:` to the end of the line
and remove the `#` if present, so that it looks similar to this. (NOTE: the
directories listed on different machines might be in a different order or
contain different directories altogether).

```{bash}
#| eval: false

# the line with ":/usr/local/bin/" appended
Defaults secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin/"
```

After saving this file, running `sudo singularity` should no longer produce a
command not found error.


### Autocompletion

Create an auto-completion file to enable Tab-completion when typing singularity
commands in the bash command line.

```{bash}
#| eval: false

# Generate completion script
singularity completion bash > singularity

# Move it to the correct directory
sudo mv singularity /etc/bash_completion.d/singularity
```

Images (a `.sif`-file) can either be build locally using the `singularity buid`
command in conjunction with a configuration file (`.def`-file) or pulled from
Docker-Hub, an online resource that makes a large variety of images available
for users.

```{mermaid}
flowchart LR
A(.def-File) --> B(.sif-Image)
C[Docker Hub] --> B(.sif-Image)

```

The first example in the Singularity documentation is to run an image from
Docker hub, the infamous lolcow. The `--debug`-flag creates verbose output. The
`singularity run`-command does not download the image into the local working
directory, but into `.singularity/cache`.

```{bash}
#| eval: false

# display the content of the .singularity directory prior to running the command 
tree .singularity/cache

# Download and run the image from docker hub
singularity --debug run library://lolcow

# display the content of the .singularity directory after running the command 
tree .singularity/cache
```

The cache can be inspected using the `singularity cache list|clean` commands.
Images are named after their SHA256-hash, which can make managing many images in
cache confusing.

```{bash}
#| eval: false

# lists total number of containers in cache and the occupied space
singularity cache list

# deletes all files currently in the `.singularity/cache` folder, while 
# retaining the directories.
singularity cache clean
```

## Usage

### Pull a docker image from Docker Hub

A library with R-specific docker images can be found at
[Rocker](https://www.rocker-project.org/images/). Their website also includes an
introduction to [Singularity](https://www.rocker-project.org/use/singularity).
The following command pulls and converts an
[image](https://www.rocker-project.org/images/base/r-devel) from the docker hub,
a repository for pre-build docker images (`SIF` = Singularity Image Format) from
Rocker. Specifications of each image can be found on the respective websites.
The advantage of this is to be able do create a singularity container without
specifying a .def file. The large amount of pre-configured images allows users
to quickly download containers that come with pre-installed applications for a
given task. This will download a file called `r-devel_latest.sif`, a singularity
container file into the current working directory.

```{bash}
#| eval: false

# Images can also be searched via this command. 
singularity search rocker

# Download the latest version of the r-devel Rocker Image from Docker Hub
singularity pull docker://rocker/r-devel

# Instead of downloading, the image can be build using the local singularity installation
singularity build r-devel-built.sif docker://rocker/r-devel
```

The dockerfile for this image can be retrieved from
[GitHub](https://raw.githubusercontent.com/rocker-org/r-devel/master/Dockerfile).
The following chunk downloads the respective dockerfile and deposits it in the
folder `resources`.

```{bash}
#| eval: false

# Download the docker file from github
wget https://raw.githubusercontent.com/rocker-org/r-devel/master/Dockerfile

# move it into a seperate folder
mkdir resources
mv Dockerfile resources/r_devel_dockerfile
```

The file can be inspected to see the configuration for the downloaded image.
Dockerfiles are however not compatible with singularity and cannot be used to
build the respective image (`.sif`-file). Singularity image build recipes are
called definition files and end on `.def`. Documentation is again available
through the [Sylabs Singularity
documentation](https://docs.sylabs.io/guides/3.10/user-guide/definition_files.html).

```{bash}
#| eval: false

cat ../resources/r_devel_dockerfile
```

### Run Image

Users can interact with containers in several ways: Run (`singularity run`) a
pre-specified command (in the `%runscript` section of the definition file),
execute (`singularity exec`) and start a shell from within the container
(`singularity shell`). Using the `shell` command it can be seen that the content
of the users' home directory (`/home\$USER`) can still be accessed when running
the container. This is one of the mounted directories from the host machine.


```{bash}
#| eval: false

# This command runs the command prespecified in the image. In the case of this container, it simply starts a new R-session.
singularity run r-devel_latest.sif

# This command runs a user-specified command, in this case a call to R that generates 100 normally distributed numbers.
singularity exec r-devel_latest.sif R --slave -e "rnorm(1e2)"

# This command opens a shell n the container environment. 
# Notice how the prompt of the shell changes to something like `Singularity>`,
# but `ls` still returns the contents of your host machine 
# (i. e. /home/YOUR_USER/ ). This can also be run as root or non-root
singularity shell r-devel_latest.sif

# Finally, a container can also be executed as file
./r-devel_latest.sif
```

### The definition file

From scratch, a SIF-file is built using a SingularityCE definition (`.def`)
file. This way, environment variables, libraries and files can be added to a
base-container. Specifications of the singularity definition file can again be
found in the [official
documentation](https://docs.sylabs.io/guides/3.10/user-guide/definition_files.html#definition-files).
It consists of several sections that define the build-instructions. The main
parts are the Header and the Body.

*   Header: Specify the registry/library and the base container to be used for building the image
*   Body: Specify all installation instructions like libraries, environment variables, meta-data and files to be copied into the container etc.

The body contains several sections, each with specific tasks to be executed during the build process. A description of each section can be found [here](https://docs.sylabs.io/guides/3.10/user-guide/definition_files.html#sections). Our first test-SIF file will contain the following sections.

*   `%post`: The section contains instructions to be executed at build time. It is used to install software to be used by the container. It can also be used to set environment variables that are set at build time. In this example, it is the date and time when the build command is run. The command is saved in the file `/.singularity.d/env/91-environment.sh` and sourced when starting the container. 
*   `%environment`: Define environment variables to be set at runtime. In this example, it is the date and time when the container is run. 
*   `%runscript`: Define commands to be run when the container is executed via `singularity run`. In this case, print the date of container creation and start a R session.
*   `%labels`: Use to add meta data like author, version etc.
*   `%help`: Add information meta-data to be displayed when `singularity run-help` is run.

### Our first DEF and SIF

For an introduction, we can build our own `.sif`-image based on the light-weight
docker image [Alpine](https://hub.docker.com/_/alpine). The header information
about using alpine from docker hub can be found
[here](https://docs.sylabs.io/guides/3.10/user-guide/appendix.html#build-docker-module).
Then we will update the container library and install R using the alpine package
manager `apk` and specify a runscript to start R when using `singularity run`.
We will not set any environment variables and leave the field empty. The
following chunk will create the `alpine_min_r.def` file in the current working
directory.

```{bash}
#| eval: false

echo \
"BootStrap: docker
From: alpine:latest

%post
apk update
apk upgrade
apk add R
NOW=\`date\`
echo \"export NOW=\\\"\${NOW}\\\"\" >> \$SINGULARITY_ENVIRONMENT

%environment
export AUTHOR=\"Carl\"

%runscript
echo \"Container creation date: \$NOW\"
echo \"Container run date: \`date\`\"
echo \"Container creator: \$AUTHOR\"
for i in 3 2 1; do echo \"Starting R in \$i...\"; sleep 1; done
R

%labels
Author: AG GenStat @ IMISE

%help
This is a first test container. It starts a R session when executed." \
>> alpine_min_r.def
```

Saving a file containing this chunk under e. g. `alpine_min_r.def`, it can be
now be build using by providing the name of the SIF file and the definition file
using `singularity build IMAGE_NAME.sif alpine_min_r.def`. This will create the
SIF file in the current directory.


```{bash}
#| eval: false

# This needs to be run as root
singularity build alpine_min_r.sif alpine_min_r.def

```

We can now interact with with the newly-created container. 

### Investigate Image

Several commands can be used to inspect properties of a .sif file (singularity
image file).

```{bash}
#| eval: false

# Displays some meta-data about the image, such as author, licence, build-date
singularity inspect alpine_min_r.sif

# Print the text supplied under %help in the DEF
singularity run-help alpine_min_r.sif
singularity inspect --helpfile alpine_min_r.sif

# List data objects in a SIF file
singularity sif list alpine_min_r.sif

# Displays info of image header with some meta-data about the container
singularity sif header alpine_min_r.sif

# Print information about the data objects
singularity sif info 1 r-devel_latest.sif
singularity sif info 2 r-devel_latest.sif
singularity sif info 3 r-devel_latest.sif
singularity sif info 4 r-devel_latest.sif
```

### Building mutable images

A SIF is by default read-only, so a built image cannot be changed. Instead of
creating a binary image file, the container can also be build as a browsable
folder using the `--sandbox` option. Using this option, containers can be
modified as root user. SIFs cannot be changed, e. g. when using `singularity
shell` as root user. Using a sandbox can be useful to modify images without the
need to recurrently rebuild images. However, this should only be used for
development and testing, as any permanent changes to an image should be
specified in the image definition file. These sandbox can then be build into a
.sif using the `singularity build` command.

```{bash}
#| eval: false

# create new image in folder 
sudo singularity build --sandbox alpine_min_r_sandbox alpine_min_r.def

# change directories into the newly created sandbox and inspect it
cd alpine_min_r_sandbox
ls -hartl

# The folder acts as an image file and all singularity commands, e. g. shell, 
# exec, and run will work without any need to modify commands.

# build the sandbox into a .sif
singularity build alpine_min_r.sif alpine_min_r_sandbox
```

### Environment variables

Setting and manipulating environment variables in a container is a convenient
feature for creating settings. They can be set in the DEF, as well as when
starting a container. Useful flags are:

*   `--env` flag: Set variables when calling `exec/run/shell` in the form of `singularity exec --env VARIABLE1=VALUE1, VARIABLE2=VALUE2 alpine_min_r.sif`
*   `--env-file` flag: Set variables from a text file in the form `VARIABLE=VALUE`

Running the container with the flag `--cleanenv` will retain none but the
essential variables in the container environment. The default variables are
documented
[here](https://docs.sylabs.io/guides/3.10/user-guide/environment_and_metadata.html).
Environment variables set in a container can be quickly searched using the `env`
command in conjunction with `grep`. For example, searching all
singularity-specific variables, i. e. those prefixed with `SINGULARITY_` can be
accomplished using this command:

```{bash}
singularity exec alpine_min_r.sif env | grep ^SINGULARITY
```

This also lists `SINGULARITY_ENVIRONMENT`, specifying the variables set in the
`%post` section of the DIF.

**Important**: Environment variables set in the `%environment` are not set
during build time. Variables necessary during building an image need to be set
in the `%post` section of the DEF.

A specific feature is the ability to manipulate `$PATH`, the variable specifying
where the system looks for executable applications. This is important when a
program is installed as a non-root user and the application binary is created in
a folder not in `$PATH`. Ways to specify non-default paths are documented
[here](https://docs.sylabs.io/guides/3.10/user-guide/environment_and_metadata.html#manipulating-path).

### Files management and mounting of directories

We want to use containers to contain, read and write files in order to properly
execute analysis modules. Creating files is straight-forward. Several
directories on the host machine are mounted when running a container. The
include the users’ home directory (i. e. `/home/YOUR_USER`) among others. These
are documented
[here](https://docs.sylabs.io/guides/latest/user-guide/bind_paths_and_mounts.html).
Directory mounts can be enabled and disabled for each container. It could for
example be useful to mount a directory containing data needed for a specific
analysis module that is saved outside of the users’ home directory. In the first
chunk, the working directory when running a container is printed to the console.

```{bash}
# Show the directory the container is executed from. Files will simply be saved
# here.
singularity exec alpine_min_r.sif pwd
```

With these commands, we will write a R data object (`.RData`) to our host
filesystem. This file is persistent and will exist after a container is closed.

```{bash}
# execute command from image. We will create a file in the local host directory
# that we can use later to learn how to handle files in SIFs
singularity exec alpine_min_r.sif R --slave -e "data(iris);save(iris,file='rDataFile.RData')"

# check for the newly created file
ls -hartl | grep -e RData
```

In addition to the standard mounted paths, additional directories can be mounted
using the `--bind` or `--mount` flags. The `--mount` flag expects specifications
in the format `type=bind,src=<source>,dst=<dest>`, with comma-delimited options.
The `source` is the directory on the host machine (the machine running the
container) and `dst` is the destination mount point on the container filesystem.
With the `ro` option, directories can be mounted as read-only, which is
especially useful for directories containing analysis data. With the following
command, the directory `/home\$USER` (i. e. the current user's home directory)
on the host will be mounted as read-only directory in the container. This is
demonstrated by trying to create (`touch`) an empty file in the target
directory. The system will refuse the operation.

```{bash}
singularity exec \
--mount type=bind,source=/home\$USER,dst=/home\$USER,ro \
my_container.sif touch /home\$USER/my_testfile
```

This can also be specified using the environment variable `$SINGULARITY_MOUNT`.
Multiple mount points can be separated by newlines (`\\n`). The variable can be
persistently set in the user's `.bashrc`. Other useful flags for that impact the
default mounts are:

*   `--no-home`: Mounts the current working directory instead of `$HOME` when running the container. When 
*   `--containall` or `-C` binds a dummy home directory and not the host home. Files created in this directory are not persistent when leaving the container. 

### Using input files and handling output

We want to add analysis scripts as R-scripts or snakemake files to containers to
use them as isolated analysis modules. Therefore, in addition to programs and
libraries necessary for the analysis, the analysis scripts need to be saved in
the container. Files to be added to an image can be specified in the definition
file in the `%files` section. The details are documented
[here](https://docs.sylabs.io/guides/latest/user-guide/definition_files.html#files).
For complicated analyses, the input needs to be highly standardized and tested
and output needs to be saved in a directory that is accessible for the user but
should still be read-only. Re-using the `alpine_min_r.def` from above, the
following DEF contains an additional section `%files`, that instructs
singularity to package the file `rDataFile.RData` into the image file. The
instructions are given in the format `<source> [<destination>]`. The source file
is mandatory, the destination optional, since the path of the source file will
be mirrored if no destination is given. `rDataFile.RData`, currently in the
working directory, will be copied to `/data` in the container's root directory.
Additionally, a small`main.R` script using the copied file will be created under
the `/scripts/` directory that will be executed when running the container that
loads the saved `rDataFile.RData`. It is also possible to copy multiple files
using a pattern-matching syntax documented
[here](https://docs.sylabs.io/guides/latest/user-guide/definition_files.html#copying-multiple-files-with-patterns).

```{bash}
# This creates the analysis script in the current directory
echo \
"# Load the data file from within the container from the directory specified in the %files section
load(\"/data/rDataFile.RData\")

# Create some output
summary(iris)

# Do some kind of computation on the data and return output
print(paste0(\"The mean sepal length is:\", round(mean(iris\$Sepal.Length), 2)))

# Finish up and provide info about computing environment
print(\"Finishing analysis....\")
sessionInfo()" \
> main.R

# This created the DEF
echo \
"BootStrap: docker
From: alpine:latest

%post
apk update
apk upgrade
apk add R
NOW=\`date\`
echo \"export NOW=\\\"\${NOW}\\\"\" >> \$SINGULARITY_ENVIRONMENT

%files
rDataFile.RData /data/
main.R  /scripts/

%environment
export AUTHOR=\"Carl\"

%runscript
echo \"Container creation date: \$NOW\"
echo \"Container run date: \`date\`\"
echo \"Container creator: \$AUTHOR\"
Rscript --verbose /scripts/main.R

%labels
Author: AG GenStat @ IMISE

%help
This is a first test container. It runs a small R script when executed." \
> alpine_min_r.def
```

Building and running the container (using the `-C` flag to not mount the home
directory) will execute the `main.R` script and produce the desired output.

```{bash}
# build the recipe
singularity build alpine_min_r.sif alpine_min_r.def

# without mounting anything
singularity run -C alpine_min_r.def
```

## A first minimum example analysis module

In this section, we will create a container using the features explored above.
The container should contain a simple analysis script, read files from a mounted
read-only directory and save results, including a log of the analysis output to
a user-specific folder on the host filesystem.

```{bash}
# create the directories to mimic the (imaginated) host machine our
# container will run on
mkdir -p singularity_host/data
mkdir singularity_host/results
mkdir singularity_host/scripts

# move the input data (which might be a huge file) to a central directory on
# the host machine and not into the container
mv rDataFile.RData singularity_host/data/Data_01.RData

# this results in a clean host directory structure including folders for the 
# data, the analysis scripts and the results
tree singularity_host
```

We can use bash commands to prepare our system for this new container.
Specifically, we must create a unique and writable result folder and create the
environment variable specifying the mount point for the `data` and `results`
folders.

```{bash}
# create the name for the result folder based on the username and some
#randomness to avoid collisions
RAND_PART=`head -c 500 /dev/urandom | tr -dc 'A-Z0-9' | head -c 5`
FOLDER_NAME=${USER}_${RAND_PART}

# make the FOLDER_NAME variable accessible for the shell 
export FOLDER_NAME

# since randomness is involved, save the results folder name in a file in the user's home
echo $FOLDER_NAME > /home\$USER/.env

# The folder will be the mount point for the user's results
mkdir /home\$USER/TutorialContainers/singularity_host/results\$FOLDER_NAME

# specify the mount points as environment variable for when running the container
export SINGULARITY_BIND="singularity_host/data:/mnt/data:ro,singularity_host/results/'$FOLDER_NAME':/mnt/results"

# this should work but doesn't
# export SINGULARITY_MOUNT=$'type=bind,src=singularity_host/data,dst=/mnt/data,ro\ntype=bind,src=singularity_host/results/'$FOLDER_NAME',dst=/mnt/results'
```

**NOTE:** Setting the environment variable `$SINGULARITY_MOUNT` presents some
problems. Multiple mounts should be separated by a newline `"\\n"`, but it still
failed for this example. Therefore, the variable `$SINGULARITY_BIND` was used
that demands a different syntax in the form `<source>:<destination>:<option>`
and different mounts can be separated by a comma.

When running this command as a script, it is important not to run the script
using `./myscript.sh`, because this will start a new shell within our executing
shell and the environment variables will not be set. Instead, using the
dot-notation or `source` will execute the commands in the current shell. Now the
container DEF and the analysis can be specified. In this DEF, we also install
additional libraries in the container in the `%post%` section, namely the
`data.table` package. Note that additional system libraries (`apk add zlib
zlib-dev pkgconfig R-dev make openmp`) need to be installed in order for
`data.table` to install successfully.

```{bash}
echo \
"BootStrap: docker
From: alpine:latest

%post
apk update
apk upgrade
apk add zlib zlib-dev pkgconfig R R-dev make openmp
NOW=\`date\`
echo \"export NOW=\\\"\${NOW}\\\"\" >> \$SINGULARITY_ENVIRONMENT

# Install necessary libraries
R --slave -e 'install.packages(\"data.table\", repos=\"https://cloud.r-project.org/\")'

%files
singularity_host/scripts/Analysis_01.R  /scripts/main.R

%runscript
echo \"Container creation date: \$NOW\"
echo \"Container run date: \`date\`\"
echo \"Container creator: \$USER\"
Rscript --vanilla --verbose /scripts/main.R > /mnt/results/Analysis_01.Rout

%labels
Author: AG GenStat @ IMISE

%help
This is a minimal example container. It utilizes mounts and runs a small R script when executed." \
> alpine_analysis_01.def
```

The analysis script (`Analysis_01.R` on the host, simply `main.R` in the
container) should read the mounted file, do some computations and save some
results that can be accessed once the container finished the analysis. Starting
the scripts using `Rscript` is the successor of `R CMD BATCH` and also accepts
common flags:

* `--vanilla`: Combine --no-save, --no-restore, --no-site-file --no-init-file and --no-environ
* `--verbose`: Print information on progress (can be piped into file)
* `--slave`: Run as quietly as possible (when a subprocess and no output is required)

```{bash}
echo "\
library(data.table)

# This data is loaded from the mounted directory on the host machine
load(\"/mnt/data/Data_01.RData\")

# use the loaded library
setDT(iris)

# Create some output
summary(iris)

# create a summary table
res <- iris[, .(mean_sepal_length = mean(Sepal.Length),
                max_sepal_width = max(Sepal.Width),
                min_petal_length = min(Petal.Length),
                total_petal_width = sum(Petal.Width)
                ), by = Species]

# save the result file to the results folder
fwrite(x = res,file = \"/mnt/results/Results_01.csv\", sep = \",\")

# Finish up and provide info about computing environment
print(\"Finishing analysis....\")
sessionInfo()"\
>   singularity_host/scripts/Analysis_01.R

# check the structure of our host directory
tree singularity_host
```

Now the DEF as well as the analysis are ready and the container can be built.

```{bash}
# create the image (needs to be run as root using `sudo`)
singularity build alpine_analysis_01.sif alpine_analysis_01.def

# run the image 
singularity run alpine_analysis_01.sif

# check whether output was produced
tree sigularity_host

# Two files were created, first a log of the script (a html report would be nicer)
cat singularity_host/results/Analysis_01.Rout

# The second file is a csv with the results from the analysis
cat singularity_host/results/Results_01.csv
```

## Configuration

###  Application base configuration

Configuration of Singularity itself is documented in the [administration
guide](https://docs.sylabs.io/guides/latest/admin-guide/configfiles.html). The
configuration is carried out on the host and is important for the general
behaviour of the program. When installing Singularity from source, the main
configuration is root-owned and located at
`/usr/local/etc/singularity/singularity.conf`. The options are documented
[here](https://docs.sylabs.io/guides/latest/admin-guide/configfiles.html#singularity-conf).
Important options include the ability to set standard mount points, like `/tmp`
and `/proc` for running containers. Options can be chosen to be maximally
restrictive without creating problems during container runtime.

### Container resource management

Even though schedulers in a HPC environment limit a processes computing
resources, it might sometimes be useful to do this on the individual container
level. The process is documented
[here](https://docs.sylabs.io/guides/latest/user-guide/cgroups.html).

### MPI applications & GPU support

Message Passing Interface (MPI) describes a standard for distributed memory
parallelization, i. e. allowing for parallelization of computations on a HPC.
The open-source implementation is called [Open MPI](https://www.open-mpi.org/).
While on a single node (e. g. a single computer), parallelization does not
require the use of MPI, a HPC consists of several servers (i. e. computers) that
are connected to form the compute environment. Each of these servers are called
a node. Each node has one or multiple CPUs with multiple (physical) cores and
mostly two (virtual) threads per core. Orchestrating parallel computations
across nodes requires additional effort. The process of coordinating these
computations across nodes is handled by MPI. An introduction to the complexities
of high-performance computing can be found
[here](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial).

In the Singularity context, the aim is to allow containers to employ multiple
threads and cores for started subprocesses. Given MPI support and configuration,
containers can be run using `mpirun singularity run` instead of `singularity
run`.  Documentation is available
[here](https://docs.sylabs.io/guides/latest/user-guide/mpi.html). Two models are
available to instantiate MPI code with singularity containers:

* *Host MPI/Hybrid Model*: Depends on the host instance of MPI, the MPI instance in the container and works with resource managers like SLURM. Depends on compatible versions of MPI in the container and the host, therefore it is advisable to build a separate MPI container
* *Bind Model*: Depends only on the host MPI installation and the container accesses the executable file through a directory mount/bind. This mode also works with resource managers.  

Singularity container also support computations running on GPUs. Details are
documented [here](https://docs.sylabs.io/guides/latest/user-guide/gpu.html). Another introduction of the joint use of Singularity and MPI can be found [here](https://epcced.github.io/2021-07-29_Singularity_Online/08-singularity-mpi/index.html).

### Use with schedulers (SLURM)

Singularity integrates seamlessly with schedulers like SLURM. Documentation
about this process can be found  in the SLURM documentation
[here](https://slurm.schedmd.com/quickstart.html).

### Security

It is important to restrict users as much as possible when operating on a shared
environment as a HPC. Luckily, most of these restrictions (e. g. default umask)
are handled by the host itself by setting user and group rights as well as
managing file permissions. Singularity is designed to run using the same
permissions as the user executing the container.

Another important point is the trust in the container itself. Singularity
supports cryptographic signatures and enforcement of valid signatures on the
containers to be executed on the host. Containers can be signed at during
building and signatures checked before running by configuring an execution
control list (ECL). Details are documented
[here](https://docs.sylabs.io/guides/latest/user-guide/signNverify.html).

Additionally, containers can be encrypted using a pass phrase or an RSA key.
Details are documented
[here](https://docs.sylabs.io/guides/latest/user-guide/encryption.html). This
makes containers only accessible to users with the correct pass phrase or private
key.

### TODO How to set container user with less priviliges

## A more complex workflow

### Snakemake container

`Snakemake`, similarly to `Nextflow` or the R package `targets` is a domain
specific language that facilitates the organisation of data analysis workflows
with numerous desirable features for larger projects. The official website is
found [here](https://snakemake.github.io/) and the documentation including an
introduction is found
[here](https://snakemake.readthedocs.io/en/latest/index.html). One summary from
the official documentation is as follows:

> One main advantage is that snakemake automatically tracks all dependencies
> within a project and executes necessary steps according to this dependency
> graph. This means that only those steps are executed that are necessary to
> produce missing results or recreate those that are older than the inputs. So
> when a step in the middle of a larger workflow consisting of multiple steps is
> changed, only results downstream of this changed script will be changed, because
> potentially previously existing results are now older than the analysis script,
> thus requiring an update.

In order to use Snakemake in a container to structure an analysis workflow, it
needs to be installed in the analysis container. Snakemake depends on the
`Python` toolchain and a package manager like `Conda`. It is desirable to keep
the base container as small as possible, therefore a minimal implementation of
`Conda` such as `Miniconda` can be installed. Since alpine Linux misses `glibc`
and the `Miniconda` installation fails as a result, a small Debian-based
container can be used as a basis for this build instead. Examples of dockerfiles
installing Snakemake can be found found
[here](https://github.com/snakemake/snakemake/blob/main/Dockerfile) and
[here](https://github.com/datarevenue-berlin/alpine-miniconda/blob/master/Dockerfile).
A workflow should be organized per [this template](https://github.com/snakemake-workflows/snakemake-workflow-template).

The following definition file creates a snakemake-capable container:

```{bash}
echo "\
BootStrap: docker
From: bitnami/minideb:latest

%environment
export PATH=\"/opt/conda/condabin:/opt/conda/bin\${PATH:+:\${PATH}}\"

%post

# set variables for buildtime
export CONDA_DIR=\"/opt/conda\"
export PATH=\"\$CONDA_DIR/bin:\${PATH}\"
export PATH=\"\$CONDA_DIR/envs/snakemake/bin:\${PATH}\"
export MINICONDA_REPO=\"https://repo.anaconda.com/miniconda/\"
export MINICONDA_INSTALLER=\"Miniconda3-latest-Linux-x86_64.sh\"

# required system libraries
install_packages bzip2 wget ca-certificates libhts-dev

# Install Miniconda
cd /tmp & \\
wget \$MINICONDA_REPO\$MINICONDA_INSTALLER && \\
bash \$MINICONDA_INSTALLER  -b -p /opt/conda && \\
rm \$MINICONDA_INSTALLER  

# Install Snakemake toolchain (source doesnt work, use . instead)
conda update --quiet -n base -c defaults conda && \\
conda install --quiet -n base -c bioconda -c conda-forge mamba snakemake-minimal && \\
mamba update --quiet -n base -c bioconda -c conda-forge --all -y && \\
mamba clean --quiet -a -y

%files
# This section can be used to copy the Snakemake workflow

%runscript
# Running container results in executing the arguments to it
# (e.g. ./debian_snakemake.sif my_script.sh)
exec bash \"\$@\"

%labels
Author: AG GenStat @ IMISE

%help
This is a minimal example container with snakemake. It utilizes mounts and runs a small R script when executed."\
> debian_snakemake.def
```

The image is built running the image using `singularity build
debian_snakemake.sif debian_snakemake.def` (run as root with `sudo`). The
resulting image is about 200MB is size (check with `du -h
debian_snakemake.sif`). The `%runscript` is designed to execute a shell script
in the form `singularity run debian_snakemake.sif my_script.sh` or
`./debian_snakemake.sif my_script.sh`. Additionally, scripts can be run using
`singularity exec debian_snakemake.sif bash my_script.sh`. Running a script from
the snakemake environment requires some overhead. For instance, running
snakemake requires the activation of the `snakemake` conda environment. Using
the installation, a shell script executing snakemake can look like the
following.

```{r}
echo "\
#! /bin/bash

snakemake --version
" \
> my_script.sh
chmod u+x my_script.sh

# Running the script on the host machine results in an error
./my_script.sh

# executing the script using the container correctly returns the version of snakemake
./debian_snakemake.sif my_script.sh
```

### Snakemake host installation

Conversely to installing Snakemake in each container, it can be installed on the
host machine.

```{bash}
# Download the Miniconda installer, run it and delete it afterwards
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
bash miniconda.sh
rm miniconda.sh

# Update conda installation
conda update -n base -c defaults conda

# Install mamba, another package manager in the base environment
conda install -n base -c conda-forge mamba

# Install Snakemake in a separate environment
conda activate base  # Switch to environment base
mamba create -c conda-forge -c bioconda -n snakemake snakemake

# Switch to environment `snakemake` to use snakemake
conda activate snakemake # deactivate with `conda deactivate`

# Optional: Use mamba instead of conda to access the environment
mamba init
mamba activate snakemake # deactivate with `mamba deactivate`

# In Order note to activate the base environment when launching conda
conda config --set auto_activate_base false

# Manual configuration of the above
echo "export CONDA_AUTO_ACTIVATE_BASE=false" >> ~/.bashrc
```

### A workflow utilizing Singularity and Snakemake

Using snakemake, it is convenient not to package the folder containing the
workflow into the container. This would require more configuration effort,
because temporary files and logs that would normally be created within the
workflow folder would have to be re-directed to a mutable folder on the host
system outside of the container. Additionally, all environments necessary for
the analysis would need to be either pre-installed in the container or also
re-directed. Therefore it is easier to build a container with all the necessary
software, mount the workflow in the container and then execute the workflow.
Using the official snakemake [short
tutorial](https://snakemake.readthedocs.io/en/stable/tutorial/short.html),
create the necessary folder structure and download the example data.

```{bash}
# Create the working directory and download data
mkdir snakemake-demo
cd snakemake-demo
wget https://github.com/snakemake/snakemake-tutorial-data/archive/v5.4.5.tar.gz
tar --wildcards -xf v5.4.5.tar.gz --strip 1 "*/data"

# create the workflow directory and the main workflow file 
mkdir workflow
mkdir workflow/envs
touch workflow/Snakefile
```

In the main workflow file, rules for separate analysis steps are defined by
specifying inputs, outputs, the command to generate the output from the input as
well as additional configuration steps such as the virtual environment
definitions for each rule. The Snakefile will look as follows:

```{bash}
echo "\
samples = [\"A\", \"B\", \"C\"]

rule all:
    input:
        \"results/calls/all.vcf\",

rule map_reads:
    input:
        \"data/genome.fa\",
        \"data/samples/{sample}.fastq\"
    output:
        \"results/mapped/{sample}.bam\"
    conda:
        \"envs/mapping.yaml\"
    shell:
        \"bwa mem {input} | samtools view -b - > {output}\"

rule sort_alignments:
    input:
        \"results/mapped/{sample}.bam\"
    output:       
        \"results/mapped/{sample}.sorted.bam\"
    conda:
        \"envs/mapping.yaml\"
    shell:
        \"samtools sort -o {output} {input}\"

rule call_variants:
    input:
        fa=\"data/genome.fa\",
        bam=expand(\"results/mapped/{sample}.sorted.bam\", sample=samples)
    output:
        \"results/calls/all.vcf\"
    conda:
        \"envs/calling.yaml\"
    shell:
        \"bcftools mpileup -f {input.fa} {input.bam} | bcftools call -mv - > {output}\"
" >> workflow/Snakefile
```

One advantage of using snakemake is the creation of a completely pre-defined and
reproducible analysis environment. The requirements for each analysis step (or
rule) are specified in `.yaml` files in the `workflow/envs` directory.

```{bash}
echo " \
channels:
  - bioconda
  - conda-forge
dependencies:
  - bwa=0.7.17
  - samtools=1.15.1" \
  >> envs/mapping.yaml

echo " \
channels:
  - bioconda
  - conda-forge
dependencies:
  - bcftools=1.9" \
  >> envs/calling.yaml
```

This workflow can be executed using the `debian_snakemake.sif` container. This is done by specifying the results to be generated by the workflow. This can be used to request specific files, without the need to run the whole analysis. Using mount options, the directory containing the workflow can be directly mounted by the container. Then, the command to generate a result can be given to the container and Snakemake will start the commands specified in the Snakefile.

```{bash}
# Set the directory to mount the Snakemake workflow in the container
export SINGULARITY_BIND="snakemake-demo:/mnt/"

# Define a short script to start snakemake
echo "
#! /bin/bash
cd /mnt && \\
snakemake results/calls/all.vcf --cores 4 --use-conda --conda-frontend mamba
" >> start_workflow.sh

# Allow execution of the script
chmod u+x start_workflow.sh

# Observe the snakemake directory prior to running the script
tree snakemake-demo

# as a shorthand for singularity run debian_snakemake.sif start_workflow.sh
./debian_snakemake.sif start_workflow.sh

# check on the created files
tree snakemake-demo

# remove the results to try out other commands
snakemake shell debian_snakemake.sif
cd /mnt
snakemake --delete-all-output    

# test single steps by shelling into the container
snakemake shell debian_snakemake.sif
snakemake results/mapped/A.bam --cores 1
snakemake results/mapped/A.sorted.bam --cores 1
```

### A genotype calling workflow utilizing Singularity and Snakemake

We will now build a container to execute a genotype calling workflow from CEL
files that is available on the [GenStat GitHub
repository](https://github.com/GenStatLeipzig/NAKO_CallingWorkflow).

```{r}
# Download the workflow and change into the directory
git clone https://github.com/GenStatLeipzig/NAKO_CallingWorkflow
cd NAKO_CallingWorkflow

# create a directory for the output
mkdir output_test

# Edit the config file to specify correct directories of the input data and the
# analysis tools. This will be highly individual for each machine. When running
# the workflow from within the IMISE filesystem, the standard paths should work.
vim config/config.yaml

# The location of the input data
CEL_FILE_PATH: "/mnt/imise/07_programme/NAKO_Calling_Pipeline/LIFE_Adult_test"

# Where to save the results
OUTPUT_PATH: "/mnt/calling/output_test/"

# The location of the Affymetrix Power Tools binaries and configuration files
APT_PATH: "/opt/apt-2.10.2.2/bin/"
APT_ANALYSIS_PATH: "/mnt/imise/07_programme/apt/axiom_libraries/Axiom_GW_Hu_SNP.r6"
APT_GENO_QC_XML: "Axiom_GW_Hu_SNP.r6.apt-geno-qc.AxiomQC1.xml"
```

The container for this analysis will be similar to the previous build, with the
addition of the Affymetrix power tools we will ship with the SIF file.

```{r}
echo "\
BootStrap: docker
From: bitnami/minideb:latest

%environment
export PATH=\"/opt/conda/condabin:/opt/conda/bin\${PATH:+:\${PATH}}\"

%files
/net/ifs1/san_projekte/projekte/genstat/07_programme/apt/apt-2.10.2.2/ /opt/

%post

# set variables for buildtime
export CONDA_DIR=\"/opt/conda\"
export PATH=\"\$CONDA_DIR/bin:\${PATH}\"
export PATH=\"\$CONDA_DIR/envs/snakemake/bin:\${PATH}\"
export MINICONDA_REPO=\"https://repo.anaconda.com/miniconda/\"
export MINICONDA_INSTALLER=\"Miniconda3-latest-Linux-x86_64.sh\"

# required system libraries
install_packages bzip2 wget ca-certificates libhts-dev

# Install Miniconda
cd /tmp & \\
wget \$MINICONDA_REPO\$MINICONDA_INSTALLER && \\
bash \$MINICONDA_INSTALLER  -b -p /opt/conda && \\
rm \$MINICONDA_INSTALLER  

# Install Snakemake toolchain (source doesnt work, use . instead)
conda update --quiet -n base -c defaults conda && \\
conda install --quiet -n base -c bioconda -c conda-forge mamba snakemake-minimal && \\
mamba update --quiet -n base -c bioconda -c conda-forge --all -y && \\
mamba clean --quiet -a -y

%files
# This section can be used to copy the Snakemake workflow

%runscript
# Running container results in executing the arguments to it
# (e.g. ./debian_snakemake.sif my_script.sh)
exec bash \"\$@\"

%labels
Author: AG GenStat @ IMISE

%help
This is a minimal example container with snakemake. It utilizes mounts and runs a small R script when executed."\
> debian_calling.def
```

The directory mounts need to be specified as environment variable. The directory containing the workflow will be mounted at `/mnt/calling` and the base genstat directory at `/mnt/imise`

```{bash}
# Build the image
sudo singularity build debian_calling.sif debian_calling.def

# Specify workflow mount directory
export SINGULARITY_BIND="/net/ifs1/san_projekte/projekte/genstat/2208_NAKOcallingWorkflow/:/mnt/calling,/net/ifs1/san_projekte/projekte/genstat/:/mnt/imise/"

# Make sure the apt tools are executable 
chmod ug+x /net/ifs1/san_projekte/projekte/genstat/07_programme/apt-2.10.2.2/bin/*

# Start a shell from within the container
singularity shell debian_calling.sif

# change into the workflow directory of the Calling workflow
cd /mnt/calling/workflow

# Calculate all results
snakemake --configfile ../config/config.yaml -c 4 all

# check on the results
tree output_test
```

## Future additions

### Running jobs on the Uni Leipzig Galaxy Cluster

The Leipzig 

* ssh zugang
* https://www.sc.uni-leipzig.de/user-doc/quickstart/hpc/

### Report generation

* Instead of simply piping the output of the R-session to a text file, Reports can be generated...
* Implementation? Quarto, Snakemake...?
* creating a quarto doc with html reports is probably a good idea `quarto render ... --to html`
  + in separate container?

### Encryption

* Containers can be encrypted... what for? Do we need this?

### Singularity Plugins TODO

https://docs.sylabs.io/guides/latest/user-guide/plugins.html

### Caching and temporary data TODO

### Tests TODO

* section `%test`

### Benchmarking TODO

### Read The Docs TODO

### The image filesystem TODO

* singularity shell
* cd /.singularity.d
* cd /singularity
* the def file can be found at `/.singularity`
* "This example works because hostfile.txt exists in the user’s home directory. By default SingularityCE bind mounts `/home\$USER`, `/tmp`, and `$PWD` into your container at runtime."
+ might be useful to pipe arguments from singularity run/exec to Rscript...
